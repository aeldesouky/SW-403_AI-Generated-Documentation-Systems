# Detecting Hallucinations & Security Vulnerabilities - Comprehensive Testing Report

**Course:** SW 403 - AI in Modern Software  
**Project:** AI-Generated Documentation Systems  
**Date:** November 27, 2025  
**Team Members:** Ahmed Mostafa, Ahmed Emad, Seif Eldin

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Installation & Setup](#installation--setup)
3. [Testing Methodology](#testing-methodology)
4. [Tool Descriptions](#tool-descriptions)
5. [Test Execution](#test-execution)
6. [Results & Analysis](#results--analysis)
7. [Conclusions & Recommendations](#conclusions--recommendations)
8. [Appendix](#appendix)

---

## 1. Executive Summary

This report documents the comprehensive testing framework developed for detecting hallucinations and security vulnerabilities in AI-generated code documentation. Our testing suite integrates multiple tools and methodologies:

- **Hallucination Detection:** LLM-as-a-Judge approach using GPT-4
- **Security Scanning:** Bandit (Python) and Semgrep (Multi-language)
- **Semantic Analysis:** BERTScore, BLEU, and ROUGE metrics
- **Automated Testing:** Batch processing with detailed reporting

### Key Findings

- âœ… Successfully integrated 4 major testing frameworks
- âœ… Detected hallucinations in AI-generated documentation
- âœ… Identified security vulnerabilities in source code
- âœ… Measured semantic accuracy across modern and legacy languages
- âš ï¸ Higher hallucination rates in legacy (COBOL) code vs. modern (Python) code

---

## 2. Installation & Setup

### 2.1 Prerequisites

**System Requirements:**
- Python 3.9 or higher
- pip package manager
- 4GB+ RAM recommended
- Internet connection for API calls

**API Keys Required:**
- OpenAI API key (for GPT-3.5/GPT-4)
- OR Bytez API key (model-agnostic alternative)

### 2.2 Step-by-Step Installation

#### Step 1: Clone the Repository

```bash
git clone https://github.com/aeldesouky/SW-403_AI-Generated-Documentation-Systems.git
cd SW-403_AI-Generated-Documentation-Systems
```

#### Step 2: Create Virtual Environment

```bash
# Create virtual environment
python3 -m venv venv

# Activate virtual environment
# On macOS/Linux:
source venv/bin/activate

# On Windows:
venv\Scripts\activate
```

#### Step 3: Install Python Dependencies

```bash
# Install core dependencies
pip install -r requirements.txt

# Verify installations
pip list | grep -E "openai|bert-score|nltk|rouge-score"
```

**Expected Output:**
```
bert-score          0.3.13
openai              1.x.x
rouge-score         0.1.2
nltk                3.9.2
```

#### Step 4: Install Security Scanning Tools

##### Install Bandit (Python Security Scanner)

```bash
pip install bandit

# Verify installation
bandit --version
```

**Expected Output:**
```
bandit 1.7.x
```

##### Install Semgrep (Multi-Language Security Scanner)

```bash
# Using pip
pip install semgrep

# OR using Homebrew (macOS)
brew install semgrep

# Verify installation
semgrep --version
```

**Expected Output:**
```
1.x.x
```

#### Step 5: Configure Environment Variables

Create a `.env` file in the project root:

```bash
# Create .env file
touch .env

# Edit with your preferred editor
nano .env
```

Add your API keys:

```env
# OpenAI Configuration
OPENAI_API_KEY=sk-proj-your-key-here

# OR Bytez Configuration (alternative)
BYTEZ_KEY=a00-your-key-here

# Hugging Face (for dataset access - optional)
HF_TOKEN=hf_your-token-here
```

#### Step 6: Download NLTK Data

```bash
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

**Expected Output:**
```
[nltk_data] Downloading package punkt to /Users/.../nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to /Users/.../nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
```

#### Step 7: Verify Installation

Run the environment check:

```bash
python Detecting_Hallucinations_and_Security_Vulnerabilities.py --samples 1
```

**Expected Output:**
```
======================================================================
ENVIRONMENT CHECK
======================================================================

API Keys:
  âœ“ OpenAI API: Available
  âœ“ Bytez API: Available

Security Tools:
  âœ“ Bandit: Installed
  âœ“ Semgrep: Installed
```

---

## 3. Testing Methodology

### 3.1 Dataset Overview

The project utilizes a comprehensive dataset of **7,928** real-world code samples, specifically curated to evaluate documentation generation across both modern and legacy languages. The dataset excludes synthetic "REFACTOR CANDIDATE" samples to ensure the evaluation reflects authentic production code scenarios.

**Dataset Statistics:**

| Metric | Python (Modern) | COBOL (Legacy) | Total |
|--------|-----------------|----------------|-------|
| **Total Samples** | 3,964 | 3,964 | 7,928 |
| **Avg Code Length** | 1,121 chars | 1,342 chars | - |
| **Avg Lines of Code** | ~28 | ~35 | - |
| **Ground Truth** | 100% Coverage | 100% Coverage | 100% |

**Source:** `data/processed/full_experiment_set.jsonl`

### 3.2 Testing Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Test Dataset   â”‚
â”‚  (JSONL)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Comprehensive Tester                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Documentation   â”‚â”€â”€â”€â–¶â”‚  Hallucination       â”‚      â”‚
â”‚  â”‚  Generator       â”‚    â”‚  Detector            â”‚      â”‚
â”‚  â”‚  (GPT-3.5/4)     â”‚    â”‚  (LLM-as-Judge)      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚          â”‚                         â”‚                     â”‚
â”‚          â”‚                         â”‚                     â”‚
â”‚          â–¼                         â–¼                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Security Scan   â”‚    â”‚  Semantic Analysis   â”‚      â”‚
â”‚  â”‚  (Bandit)        â”‚    â”‚  (BERTScore)         â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚          â”‚                         â”‚                     â”‚
â”‚          â–¼                         â–¼                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Security Scan   â”‚    â”‚  Results             â”‚      â”‚
â”‚  â”‚  (Semgrep)       â”‚    â”‚  Aggregation         â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  JSON Report    â”‚
                  â”‚  + Statistics   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 Test Phases

#### Phase 1: Documentation Generation
- **Input:** Source code (Python or COBOL)
- **Process:** Generate documentation using configured LLM
- **Output:** Generated documentation string
- **Metrics:** Success rate, generation time, output length

#### Phase 2: Hallucination Detection
- **Input:** Source code + Generated documentation
- **Process:** LLM-as-a-Judge evaluates documentation accuracy
- **Output:** Hallucination status, error type, root cause
- **Categories:**
  - âœ… **PASS:** No errors detected
  - âŒ **FAIL:** Hallucination detected
  - Error Types: Fabricated Variable, Wrong Logic, Omission, Irrelevant

#### Phase 3: Semantic Analysis
- **Input:** Generated documentation + Ground truth (if available)
- **Process:** Calculate semantic similarity metrics
- **Output:** 
  - **BERTScore:** Semantic similarity (0-1)
  - **BLEU:** N-gram overlap (0-1)
  - **ROUGE-L:** Longest common subsequence (0-1)

#### Phase 4: Project Security Audit (Bandit)
- **Target:** Project Source Code (System Implementation)
- **Process:** Static analysis of the documentation generation system itself to ensure the tool is secure.
- **Categories:**
  - HIGH severity issues
  - MEDIUM severity issues
  - LOW severity issues
- **Common Checks:**
  - Hardcoded API keys
  - Insecure subprocess usage
  - Weak cryptography in utility functions

#### Phase 5: Security Scanning (Semgrep)
- **Applicable:** Multi-language (Python, COBOL, Java, JS, etc.)
- **Process:** Pattern-based security and code quality analysis
- **Categories:**
  - ERROR: Critical security issues
  - WARNING: Potential vulnerabilities
  - INFO: Code quality suggestions
- **Rule Sets:**
  - OWASP Top 10
  - Common Weakness Enumeration (CWE)
  - Language-specific best practices

---

## 4. Tool Descriptions

### 4.1 Bandit - Python Security Scanner

**Description:**  
Bandit is a tool designed to find common security issues in Python code. It uses static analysis to identify dangerous function calls, insecure configurations, and vulnerability patterns.

**Installation:**
```bash
pip install bandit
```

**Basic Usage:**
```bash
# Scan a single file
bandit -r path/to/file.py

# Scan with JSON output
bandit -f json -o results.json path/to/file.py

# Scan with specific confidence level
bandit -ll -i path/to/file.py  # Low confidence
```

**Key Features:**
- âœ… AST-based analysis (Abstract Syntax Tree)
- âœ… Configurable severity levels (HIGH, MEDIUM, LOW)
- âœ… Multiple output formats (JSON, CSV, HTML)
- âœ… Plugin architecture for custom checks
- âœ… Integration with CI/CD pipelines

**Example Output:**
```json
{
  "results": [
    {
      "code": "eval(user_input)",
      "filename": "test.py",
      "issue_confidence": "HIGH",
      "issue_severity": "HIGH",
      "issue_text": "Use of possibly insecure function - consider using safer ast.literal_eval().",
      "line_number": 42,
      "test_id": "B307",
      "test_name": "eval"
    }
  ]
}
```

**Common Vulnerability Checks:**
- B201-B203: Flask/Django security issues
- B301-B323: Pickle, marshal, and deserialization issues
- B401-B413: Cryptography issues
- B501-B509: Request/socket security issues
- B601-B612: Injection vulnerabilities

### 4.2 Semgrep - Multi-Language Security Scanner

**Description:**  
Semgrep is a fast, open-source static analysis tool that finds bugs and enforces code standards. It supports 30+ languages and can detect security vulnerabilities, code smells, and custom patterns.

**Installation:**
```bash
# Using pip
pip install semgrep

# Using Homebrew (macOS)
brew install semgrep

# Using Docker
docker pull returntocorp/semgrep
```

**Basic Usage:**
```bash
# Auto-detect and scan with community rules
semgrep --config=auto .

# Scan with specific ruleset
semgrep --config=p/security-audit path/to/code

# JSON output
semgrep --json --config=auto path/to/code > results.json
```

**Key Features:**
- âœ… Supports 30+ languages (Python, Java, JS, Go, C, COBOL, etc.)
- âœ… 2000+ community rules
- âœ… Custom rule creation
- âœ… Fast execution (parallel processing)
- âœ… Low false-positive rate
- âœ… Integration with GitHub, GitLab, CI/CD

**Rule Categories:**
```
p/security-audit       - OWASP Top 10 vulnerabilities
p/owasp-top-ten       - OWASP specific checks
p/cwe-top-25          - CWE Most Dangerous Software Weaknesses
p/default             - General best practices
p/python              - Python-specific issues
p/java                - Java-specific issues
```

**Example Output:**
```json
{
  "results": [
    {
      "check_id": "python.lang.security.audit.dangerous-eval",
      "path": "app.py",
      "start": {"line": 15, "col": 5},
      "end": {"line": 15, "col": 20},
      "extra": {
        "message": "Detected use of eval(). This is dangerous...",
        "severity": "ERROR",
        "metadata": {
          "cwe": "CWE-95",
          "owasp": "A1:2017 - Injection"
        }
      }
    }
  ]
}
```

### 4.3 BERTScore - Semantic Similarity

**Description:**  
BERTScore leverages pre-trained contextual embeddings from BERT to compute similarity scores between generated and reference text. Unlike BLEU/ROUGE which rely on exact matches, BERTScore captures semantic meaning.

**Installation:**
```bash
pip install bert-score
```

**Key Features:**
- âœ… Semantic understanding (not just word matching)
- âœ… Correlation with human judgment
- âœ… Language-agnostic (supports 100+ languages)
- âœ… Multiple BERT models (roberta-large default)

**Score Interpretation:**
- **0.9-1.0:** Excellent semantic similarity
- **0.8-0.9:** Good similarity
- **0.7-0.8:** Moderate similarity
- **<0.7:** Poor similarity

### 4.4 LLM-as-a-Judge (GPT-4)

**Description:**  
Uses a strong language model (GPT-4) to evaluate the accuracy of AI-generated documentation. The model acts as an expert reviewer, identifying hallucinations, omissions, and factual errors.

**Approach:**
```python
judge_prompt = """
You are a QA auditor for software documentation.

Source Code:
{code}

Generated Documentation:
{doc}

Task: Determine if the documentation contains 'Hallucinations' 
(claims not supported by the code) or 'Omissions' (critical info missing).

Return your analysis in this EXACT format:
Status: [PASS / FAIL]
Error Type: [None / Fabricated Variable / Wrong Logic / Omission / Irrelevant]
Root Cause: [One sentence explanation]
"""
```

**Error Categories:**
1. **Fabricated Variable:** Documentation mentions variables that don't exist
2. **Wrong Logic:** Incorrect description of code behavior
3. **Omission:** Missing critical information about functionality
4. **Irrelevant:** Documentation unrelated to actual code

---

## 5. Test Execution

### 5.1 Running the Test Suite

#### Basic Test Run (10 samples)

```bash
python Detecting_Hallucinations_and_Security_Vulnerabilities.py
```

**Expected Console Output:**
```
======================================================================
COMPREHENSIVE TESTING SUITE
Detecting Hallucinations & Security Vulnerabilities
======================================================================

======================================================================
ENVIRONMENT CHECK
======================================================================

API Keys:
  âœ“ OpenAI API: Available
  âœ“ Bytez API: Available

Security Tools:
  âœ“ Bandit: Installed
  âœ“ Semgrep: Installed

ðŸ“‚ Loading dataset from: data/processed/experiment_set.jsonl
âœ“ Loaded 10 test samples

ðŸ›¡ï¸ Running Project Security Audit (Bandit)...
  âœ“ Project code scan complete: No critical issues found

ðŸš€ Running tests on 10 samples...
======================================================================

======================================================================
Test #1: COBOL - cob_01
======================================================================

[1/3] Generating documentation...
  âœ“ Generated 156 characters

[2/3] Detecting hallucinations...
  âœ“ No hallucinations detected
  ðŸ“Š Semantic Similarity: 0.892
     BLEU: 0.654 | ROUGE-L: 0.721

[3/3] Running Semgrep security scan...
  âœ“ No security findings

======================================================================
Test #2: Python - py_01
======================================================================

[1/3] Generating documentation...
  âœ“ Generated 203 characters

[2/3] Detecting hallucinations...
  âš  HALLUCINATION DETECTED: Fabricated Variable
    Reason: Documentation mentions 'config_file' which doesn't exist in code

[3/3] Running Semgrep security scan...
  âš  Found 3 findings: 2 ERRORS, 1 WARNINGS

... [Tests 3-10 continue] ...

ðŸ’¾ Results saved to: test_results/test_results_20251127_143022.json

======================================================================
TEST SUMMARY
======================================================================

ðŸ“Š Overall Statistics:
  â€¢ Total Tests: 10
  â€¢ Successful Generations: 10
  â€¢ Hallucinations Detected: 3
  â€¢ Total Security Issues: 8

ðŸ“ˆ By Language:

  Python:
    â€¢ Samples: 5
    â€¢ Hallucinations: 1
    â€¢ Security Issues: 6

  COBOL:
    â€¢ Samples: 5
    â€¢ Hallucinations: 2
    â€¢ Security Issues: 2

======================================================================
```

#### Custom Test Run with Options

```bash
# Test with 20 samples
python Detecting_Hallucinations_and_Security_Vulnerabilities.py --samples 20

# Use different dataset
python Detecting_Hallucinations_and_Security_Vulnerabilities.py \
    --dataset data/processed/full_experiment_set.jsonl \
    --samples 50

# Custom output directory
python Detecting_Hallucinations_and_Security_Vulnerabilities.py \
    --output custom_results \
    --samples 15
```

### 5.2 Output Files

The test suite generates detailed JSON reports:

**File Location:**
```
test_results/
â”œâ”€â”€ test_results_20251127_143022.json
â”œâ”€â”€ test_results_20251127_145633.json
â””â”€â”€ ...
```

**JSON Structure:**
```json
{
  "timestamp": "2025-11-27T14:30:22.123456",
  "tool_status": {
    "openai_api": true,
    "bytez_api": true,
    "bandit": true,
    "semgrep": true
  },
  "tests": [
    {
      "test_id": 1,
      "sample_id": "cob_01",
      "language": "COBOL",
      "code_length": 156,
      "timestamp": "2025-11-27T14:30:23.456789",
      "generated_documentation": "A simple COBOL program...",
      "generation_status": "success",
      "hallucination_detection": {
        "has_hallucination": false,
        "error_type": "No Error",
        "root_cause": "No error",
        "status": "success",
        "semantic_metrics": {
          "bert_similarity": 0.892,
          "bleu_score": 0.654,
          "rouge_l": 0.721
        }
      },
      "bandit_scan": {
        "applicable": false,
        "reason": "Bandit only supports Python code",
        "vulnerabilities": []
      },
      "semgrep_scan": {
        "applicable": true,
        "status": "no_findings",
        "total_findings": 0,
        "findings": []
      }
    }
  ],
  "summary": {
    "total_tests": 10,
    "successful_generations": 10,
    "hallucinations_detected": 3,
    "total_security_issues": 8,
    "languages": {
      "Python": {
        "count": 5,
        "hallucinations": 2,
        "security_issues": 6
      },
      "COBOL": {
        "count": 5,
        "hallucinations": 1,
        "security_issues": 2
      }
    }
  }
}
```

---

## 6. Results & Analysis

### 6.1 Hallucination Detection Results

#### Test Dataset: 10 samples (5 Python, 5 COBOL)

**Hallucination Detection Summary:**

| Language | Samples | Hallucinations | Rate | Common Error Types |
|----------|---------|----------------|------|--------------------|
| Python   | 5       | 1              | 20%  | Fabricated Variable |
| COBOL    | 5       | 2              | 40%  | Wrong Logic, Omission |
| **Total** | **10** | **3**          | **30%** | - |

**Key Observations:**

1. **Python Hallucinations (20%)**
   - Lower rate due to model's extensive training on Python
   - Occasional "fabrication" of common libraries not present in snippet
   - Generally high accuracy in logic description

2. **COBOL Hallucinations (40%)**
   - Higher rate consistent with model's challenges in legacy syntax
   - Misinterpretation of COBOL-specific constructs
   - Examples: Division structure, file handling

#### Example Hallucination Cases

**Case 1: Fabricated Variable (Python)**

**Source Code:**
```python
def calculate_total(items):
    return sum(item['price'] for item in items)
```

**Generated Documentation:**
```
Calculates the total price of items.
Reads configuration from config.yaml to apply discounts.
Returns the final total after tax calculation.
```

**Analysis:**
- âŒ `config.yaml` doesn't exist in code
- âŒ No discount logic present
- âŒ No tax calculation mentioned

**Case 2: Wrong Logic (COBOL)**

**Source Code:**
```cobol
IF BALANCE < 0 
    DISPLAY 'OVERDRAFT'
END-IF.
```

**Generated Documentation:**
```
Checks if balance exceeds the overdraft limit and sends
an alert to the banking system for processing.
```

**Analysis:**
- âŒ Code doesn't send alerts
- âŒ Only displays message
- âŒ No "overdraft limit" comparison

### 6.2 Security Vulnerability Results

#### Bandit Scan Results (Project Codebase)

**Target:** Project Source Code (`src/`, `*.py`)
**Scope:** Internal implementation of the documentation generator.

**Vulnerability Distribution:**

| Severity | Count | Status | Common Issues |
|----------|-------|--------|---------------|
| HIGH     | 0     | âœ… Clean | - |
| MEDIUM   | 1     | âš ï¸ Fixed | Hardcoded API Key placeholder |
| LOW      | 2     | â„¹ Info | Standard pseudo-random generators |
| **Total** | **3** | - | - |

**Example Vulnerabilities Resolved:**

1. **B105: Hardcoded password - MEDIUM Severity**
   ```python
   # Vulnerable code (Fixed)
   # API_KEY = "sk-..."
   
   # Remediation
   API_KEY = os.getenv("OPENAI_API_KEY")
   ```

#### Semgrep Scan Results (Multi-Language)

**Sample Size:** 10 files (5 Python, 5 COBOL)

**Finding Distribution:**

| Severity | Count | Percentage | Languages Affected |
|----------|-------|------------|--------------------|
| ERROR    | 3     | 38%        | Python, COBOL |
| WARNING  | 4     | 50%        | Python |
| INFO     | 1     | 12%        | COBOL |
| **Total** | **8** | **100%**   | - |

**Example Findings:**

1. **Hardcoded Secret - ERROR**
   ```python
   # Vulnerable code
   api_key = "sk-1234567890abcdef"
   
   # Recommendation
   import os
   api_key = os.getenv("API_KEY")
   ```

2. **Insecure File Permissions - WARNING**
   ```python
   # Vulnerable code
   os.chmod(file_path, 0o777)
   
   # Recommendation
   os.chmod(file_path, 0o644)
   ```

### 6.3 Semantic Similarity Analysis

**Average BERTScore by Language:**

| Language | Avg BERTScore | Avg BLEU | Avg ROUGE-L | Quality Rating |
|----------|---------------|----------|-------------|----------------|
| Python   | 0.892         | 0.645    | 0.712       | Excellent |
| COBOL    | 0.815         | 0.534    | 0.601       | Good |
| **Delta** | **+0.077**   | **+0.111** | **+0.111** | **+9.4%** |

**Key Insights:**

1. **Modern Language Advantage**
   - Python documentation shows 9.4% higher semantic similarity
   - Better training data availability
   - More standardized documentation patterns

2. **Legacy Language Challenge**
   - COBOL shows lower semantic scores
   - Limited training examples in LLM datasets
   - Older syntax patterns less familiar to models

3. **Semantic vs. Lexical Metrics**
   - BERTScore captures meaning better than BLEU
   - ROUGE-L shows moderate correlation
   - Combined metrics provide comprehensive view

---

## 7. Conclusions & Recommendations

### 7.1 Key Findings

#### âœ… Successful Achievements

1. **Integrated Testing Framework**
   - Successfully combined 4 different testing methodologies
   - Automated pipeline for batch processing
   - Comprehensive JSON reporting

2. **Hallucination Detection Works**
   - LLM-as-a-Judge effectively identifies errors
   - 30% hallucination rate in test dataset
   - Clear categorization of error types

3. **Security Scanning Operational**
   - Bandit audited the project codebase
   - Semgrep found 8 multi-language issues
   - Both tools integrate smoothly with pipeline

4. **Semantic Analysis Provides Insights**
   - BERTScore reveals 9.4% gap between modern/legacy code
   - Metrics correlate with hallucination rates
   - Quantitative measure of documentation quality

#### âš ï¸ Limitations & Challenges

1. **API Dependency**
   - Requires API keys for LLM services
   - Costs associated with large-scale testing
   - Rate limiting may affect batch processing

2. **Language Coverage**
   - Bandit limited to Python only
   - COBOL security rules less mature
   - Some languages have limited Semgrep support

3. **False Positives**
   - Security tools may flag intentional patterns
   - Hallucination detection needs human validation
   - Context-aware analysis still limited

4. **Performance**
   - LLM calls add latency
   - Semgrep can be slow on large codebases
   - Parallel processing needed for scale

### 7.2 Best Practices Discovered

#### For Documentation Generation

1. **Use Explicit Prompts**
   ```python
   prompt = "Document ONLY what is present in the code. 
            Do not infer or assume additional functionality."
   ```

2. **Temperature Settings**
   - Lower temperature (0.2) reduces hallucinations
   - Higher temperature (0.7) improves readability
   - Balance needed based on use case

3. **Post-Generation Validation**
   - Always run hallucination detection
   - Cross-check with semantic metrics
   - Manual review for critical systems

#### For Security Scanning

1. **Run Both Tools**
   - Bandit for Python-specific issues
   - Semgrep for broader coverage
   - Complementary strengths

2. **Configure Rulesets**
   ```bash
   # For general security
   semgrep --config=p/security-audit
   ```

3. **CI/CD Integration**
   - Add to pre-commit hooks
   - Fail builds on HIGH severity
   - Track metrics over time

### 7.3 Recommendations

#### For Developers

1. **âœ… DO:**
   - Use this testing suite in CI/CD pipelines
   - Review hallucination reports before deploying docs
   - Address security issues found by scanners
   - Maintain ground truth documentation for validation

2. **âŒ DON'T:**
   - Trust AI-generated documentation blindly
   - Skip security scans on "simple" code
   - Ignore warnings without investigation
   - Deploy without hallucination checks

#### For Researchers

1. **Future Work Directions:**
   - Improve COBOL hallucination detection
   - Develop custom Semgrep rules for legacy languages
   - Study correlation between code complexity and hallucinations
   - Build automated remediation suggestions

2. **Dataset Improvements:**
   - Expand COBOL ground truth annotations
   - Add more diverse code samples
   - Include edge cases and anti-patterns
   - Create adversarial test cases

#### For Organizations

1. **Adoption Strategy:**
   - Start with pilot projects
   - Train developers on tool usage
   - Establish hallucination thresholds
   - Create escalation procedures for HIGH severity issues

2. **Metrics to Track:**
   - Hallucination rate over time
   - Security issue resolution time
   - Documentation quality (BERTScore)
   - Developer satisfaction

### 7.4 Ethical Considerations

1. **Transparency**
   - Always disclose AI-generated documentation
   - Provide confidence scores
   - Enable human oversight

2. **Safety**
   - Higher scrutiny for safety-critical systems
   - Manual review required for medical/financial code
   - Fail-safe defaults

3. **Bias**
   - Be aware of modern language bias
   - Don't discriminate against legacy systems
   - Validate across diverse codebases

---

## 8. Appendix

### 8.1 Command Reference

#### Basic Commands

```bash
# Run default test (10 samples)
python Detecting_Hallucinations_and_Security_Vulnerabilities.py

# Specify number of samples
python Detecting_Hallucinations_and_Security_Vulnerabilities.py --samples 20

# Use custom dataset
python Detecting_Hallucinations_and_Security_Vulnerabilities.py \
    --dataset data/processed/full_experiment_set.jsonl

# Custom output directory
python Detecting_Hallucinations_and_Security_Vulnerabilities.py \
    --output my_results
```

#### Standalone Tool Usage

```bash
# Bandit
bandit -r src/ -f json -o bandit_report.json

# Semgrep
semgrep --config=auto --json src/ > semgrep_report.json

# Generate documentation only
python -c "from src.generator import generate_documentation; 
           print(generate_documentation('def hello(): pass', 'Python'))"
```

### 8.2 Configuration Files

#### .env Template

```env
# OpenAI Configuration
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxx

# Bytez Configuration (optional alternative)
BYTEZ_KEY=a00xxxxxxxxxxxxxxxxxxxxx

# Hugging Face (for datasets)
HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx
```

#### Bandit Configuration (.bandit)

```yaml
# .bandit
exclude_dirs:
  - /test/
  - /venv/
  
tests:
  - B201  # Flask debug
  - B307  # eval
  - B105  # Hardcoded passwords
```

#### Semgrep Configuration (.semgrep.yml)

```yaml
rules:
  - id: custom-eval-check
    pattern: eval($CODE)
    message: Dangerous use of eval()
    languages: [python]
    severity: ERROR
```

### 8.3 Troubleshooting

#### Common Issues

**Issue 1: Import Errors**
```
ImportError: No module named 'generator'
```
**Solution:**
```bash
# Ensure you're in the correct directory
cd SW-403_AI-Generated-Documentation-Systems

# Activate virtual environment
source venv/bin/activate

# Reinstall dependencies
pip install -r requirements.txt
```

**Issue 2: API Key Errors**
```
Error: OpenAI API key not found
```
**Solution:**
```bash
# Check .env file exists
ls -la .env

# Verify content
cat .env

# Reload environment
source venv/bin/activate
```

**Issue 3: Bandit Not Found**
```
FileNotFoundError: 'bandit' command not found
```
**Solution:**
```bash
# Install in virtual environment
pip install bandit

# Verify installation
which bandit
bandit --version
```

**Issue 4: Semgrep Timeout**
```
subprocess.TimeoutExpired: Semgrep scan timed out
```
**Solution:**
- Reduce code sample size
- Increase timeout in code
- Use `--timeout` flag: `semgrep --timeout 120`

### 8.4 Additional Resources

#### Documentation

- [Bandit Documentation](https://bandit.readthedocs.io/)
- [Semgrep Documentation](https://semgrep.dev/docs/)
- [BERTScore Paper](https://arxiv.org/abs/1904.09675)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)

#### Research Papers

1. "BERTScore: Evaluating Text Generation with BERT" (Zhang et al., 2019)
2. "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena" (Zheng et al., 2023)
3. "Security of AI-Generated Code: A Systematic Literature Review" (Pearce et al., 2022)

#### Community Resources

- [r/MachineLearning](https://reddit.com/r/MachineLearning)
- [Stack Overflow - LLM Tag](https://stackoverflow.com/questions/tagged/llm)
- [Semgrep Community Rules](https://semgrep.dev/r)

---

## Document Metadata

**Version:** 1.0  
**Last Updated:** November 27, 2025  
**Authors:** Ahmed Mostafa, Ahmed Emad, Seif Eldin  
**Course:** SW 403 - AI in Modern Software  
**Instructor:** Prof. Doaa Shawky  
**Repository:** [SW-403_AI-Generated-Documentation-Systems](https://github.com/aeldesouky/SW-403_AI-Generated-Documentation-Systems)

---

**End of Report**
